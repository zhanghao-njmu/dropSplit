% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgbOptimization.R
\name{xgbOptimization}
\alias{xgbOptimization}
\title{Bayesian optimization for XGBoost.}
\usage{
xgbOptimization(
  dat,
  dat_label,
  bounds = list(),
  xgb_nfold = 5,
  xgb_nround = 20,
  xgb_early_stopping_rounds = 5,
  xgb_metric = "auc",
  xgb_thread = 8,
  opt_initPoints = length(bounds) + 1,
  opt_itersn = 10,
  opt_thread = 1,
  ...
)
}
\arguments{
\item{dat}{A \code{matrix} object or a \code{dgCMatrix} object which columns represent features and rows represent samples.}

\item{dat_label}{A vector of response classification values.}

\item{bounds}{A named list of lower and upper bounds for \code{params} in \code{\link[xgboost]{xgb.cv}}. The names of the list should be arguments passed to xgb.cv Use "L" suffix to indicate integers. A fixed parameter should be a two-length vector with the same value, i.e. bound=list(lambda = c(5, 5))}

\item{xgb_nfold}{The original dataset is randomly partitioned into nfold equal size subsamples.}

\item{xgb_nround}{Max number of boosting iterations.}

\item{xgb_early_stopping_rounds}{If NULL, the early stopping function is not triggered. If set to an integer k, training with a validation set will stop if the performance doesn't improve for k rounds. Setting this parameter engages the \code{cb.early.stop} callback.}

\item{xgb_metric}{A evaluation metric to be used in cross validation and will to be maximized. Possible options are:
\itemize{
\item \code{auc} Area under curve
\item \code{aucpr} Area under PR curve
}}

\item{xgb_thread}{Number of thread used in \code{\link[xgboost]{xgb.cv}}.}

\item{opt_initPoints}{Number of points to initialize the process with. Points are chosen with latin hypercube sampling within the bounds supplied.}

\item{opt_itersn}{The total number of times \code{xgb.cv} will be run after initialization.}

\item{opt_thread}{Number of thread used in \code{\link[ParBayesianOptimization]{bayesOpt}}.}

\item{...}{Other arguments passed to \code{\link[ParBayesianOptimization]{bayesOpt}}.}
}
\value{
A list of two object:
\describe{
\item{bayesOpt}{An object of class bayesOpt containing information about the process.}
\item{BestPars}{A list containing the parameters which resulted in the highest returned Score.}
}
}
\description{
Maximizes a xgboost evaluation metric within a set of bounds. After the function is sampled a pre-determined number of times, a Gaussian process is fit to the results. An acquisition function is then maximized to determine the most likely location of the global maximum of the user defined XGBoost evaluation metric. This process is repeated for a set number of iterations.
}
\examples{
library("xgboost")
data(agaricus.train, package = "xgboost")
dat <- agaricus.train$data
dat_label <- agaricus.train$label
bounds <- list(max_depth = c(1L, 5L), min_child_weight = c(0, 25), subsample = c(0.25, 1))
result <- xgbOptimization(dat = dat, dat_label = dat_label, bounds = bounds, opt_thread = 2)
result
}
